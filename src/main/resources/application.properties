#
# This file is part of the eskimo project referenced at www.eskimo.sh. The licensing information below apply just as
# well to this individual file than to the Eskimo Project as a whole.
#
#  Copyright 2019 - 2023 eskimo.sh / https://www.eskimo.sh - All rights reserved.
# Author : eskimo.sh / https://www.eskimo.sh
#
# Eskimo is available under a dual licensing model : commercial and GNU AGPL.
# If you did not acquire a commercial licence for Eskimo, you can still use it and consider it free software under the
# terms of the GNU Affero Public License. You can redistribute it and/or modify it under the terms of the GNU Affero
# Public License  as published by the Free Software Foundation, either version 3 of the License, or (at your option)
# any later version.
# Compliance to each and every aspect of the GNU Affero Public License is mandatory for users who did no acquire a
# commercial license.
#
# Eskimo is distributed as a free software under GNU AGPL in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License along with Eskimo. If not,
# see <https://www.gnu.org/licenses/> or write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
# Boston, MA, 02110-1301 USA.
#
# You can be released from the requirements of the license by purchasing a commercial license. Buying such a
# commercial license is mandatory as soon as :
# - you develop activities involving Eskimo without disclosing the source code of your own product, software,
#   platform, use cases or scripts.
# - you deploy eskimo as part of a commercial product, platform or software.
# For more information, please contact eskimo.sh at https://www.eskimo.sh
#
# The above copyright notice and this licensing notice shall be included in all copies or substantial portions of the
# Software.
#


# The package application properties provides default values for system configurations

# An overriding config application.properties can be put in:
# (The list is ordered by precedence - properties defined in locations higher in the list override those defined in
# lower locations).
# 1. A /config subdirectory of the current directory
# 2. The current directory
# 3. A classpath /config package
# 4. The classpath root
# or an arbitrary location can ne passed at runtime:
# java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties


# Generic Spring Application configuration
# ----------------------------------------------------------------------------------------------------------------------

# Port number to bind to (HTTP)
# The GUI is made available on this port as well as the command server used by EGMI instances to communicated with each
# others
server.port=8080

# the port number to reach remote EGMIs
# This should be the same port number as above in principle except if the EGMI master is reaching remote EGMis answering
# on different ports
remote.egmi.port=28901

# Maximum number of HTTP requests that can be answered at a time
server.tomcat.threads.max=20

# HTTP Session timeout
server.servlet.session.timeout=30m

# Use following property to put EGMI behind a specific context path in web server
# (This is usefull if a reverse proxy binds EGMI to have it under same path)
server.servlet.context-path=/egmi

# Console message upon EGMI startup and various other spring boot properties
welcome.message=Hello EGMI
application.title=EGMI

# Build and version information
build.version=@project.version@
build.timestamp=@maven.build.timestamp@
version=0.1-SNAPSHOT-DEV-1

# Size of the banner image
spring.banner.image.width=40

spring.main.allow-circular-references=true


# Application configuration
# ----------------------------------------------------------------------------------------------------------------------

# The path where the user file should be stored
conf.userFilePath=/tmp/egmi-users.json

# The number of log messages to keep for the UI
system.maxLogMessages=5000

# where to store configuration files
config-storage-path=/tmp/

# Preconfigured set of IP addresses where gluster is to be managed (coma separated)
# If these are set here, they're used as a fixed pre-defined set of data nodes to manager
# If none is defined here, zookeeper is used to track data nodes
#target.predefined-ip-addresses=192.168.10.21,192.168.10.22,192.168.10.23
target.predefined-ip-addresses=192.168.56.21,192.168.56.22,192.168.56.23,192.168.56.24

# Preconfigured set of volumes to manage
#target.volumes=spark_eventlog,spark_data,flink_data,logstash_data,test
target.volumes=spark_eventlog,spark_data,kafka_data,flink_data,flink_completed_jobs,logstash_data,kubernetes_registry

# Volumes for which the performance setting sneeds to be turned off
target.volumes.performance.off=kafka_data

# Performance settings to turn off for volumes defined in 'target.volumes.performance.off'
config.performance.off=performance.quick-read,performance.io-cache,performance.write-behind,performance.stat-prefetch,performance.read-ahead,performance.readdir-ahead,performance.open-behind

# The orchestration loop delay
system.statusUpdatePeriodSeconds=30

# Define the target minimum number of bricks we want for every volume
# It can be a fixed number (like 1, 2, 5, etc.)
# or a strategy among [ALL_NODES, LOG_DISPATCH]
# where
# - ALL_NODES    : create a brick on all nodes for every volume
# - LOG_DISPATCH : create a number of bricks being a logarithm of the number of nodes
#   + 1 node  => 1 brick
#   + 2 nodes => 1 bricks
#   + 3 nodes => 2 bricks
#   + 4 nodes => 2 + logn(4) bricks
#   + etc.
target.numberOfBricks=LOG_DISPATCH

# The ideal number of replicas to try to respect
target.defaultNumberReplica=3

# The parth on gluster nodes where gluster bricks should be stored
target.glusterVolumes.path=/var/lib/gluster/volume_bricks/


# Individual problem resolution settings
# ----------------------------------------------------------------------------------------------------------------------

# the timeout in seconds after which a node is considered definitely dead and, if applicable, EGMI will attempt to move
# his bricks elsewhere (A few minutes seems to be a good approach in practice)
problem.nodeDown.timeout=30

# the timeout in seconds after which a node is considered dead and gone and, if applicable, EGMI will attempt to remove
# it from the list of pools
problem.nodeDownRemoval.timeout=180

# the timeout in seconds after which a volume that is not existing AND not a managed volume is simply removed from the
# set of tracked volumes
problem.noVolume.timeout=60

# the timeout in seconds after which a brick offline triggers a force start of the volume
problem.brickOffline.timeout=60


# Zookeeper settings (for master election)
# ----------------------------------------------------------------------------------------------------------------------

# My own hostname or IP address in the election group
zookeeper.myId=192.168.56.1

# The URL:PORT used to connect to zookeeper (or set of URL:PORTs, coma separated)
#zookeeper.urls=192.168.10.21:2181

# Force this instance of EGMI being master ot not master, regardless of what happens in zookeeper
# Use master=true|false
master=true

# Flag this node in zookeeper as a data running node. Such nodes would register themselves as needing to be managed by
# the eskimo cluster. A node that is NOT a data node wouldn't be managed and wouldn't get any shar assigned (typically
# would only be use for master election and might be able to be elected master / orchestrator)
data=false

# The Zookeeper session timeout
zookeeper.sessionTimeout=5000

# The URL pattern used to redirect to master
# (This is used on slave - non-master - nodes to redirect to the master URL in case their US is reached instead of the
# master UI)
# Some markers are available within the pattern
# - {MASTER_NODE} : resolved to the master hostname configured on master node as "hostname" (see above)
# - {MASTER_NODE_NAME} : resolved to the master node name (hostname where dots are replaced with hyphen)
# This default settings match eskimo's expectations
master.redirect.URLPattern=/gluster/{MASTER_NODE_NAME}/egmi/app.html
